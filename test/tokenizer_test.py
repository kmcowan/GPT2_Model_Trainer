# Ensure inputs are tokenized properly
#inputs = tokenizer("Your training text here", return_tensors='pt', padding=True, truncation=True)
